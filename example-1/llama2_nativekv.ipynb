{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMET Quantization workflow for Llama 2 7B\n",
    "\n",
    "This notebook shows a working code example of how to use AIMET to quantize Llama 2 family models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Required packages\n",
    "The notebook assumes AIMET and LLamaV2 related packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != '__main__':\n",
    "    raise Exception(\"Killing multiprocessing spawn started by Converter during model preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages only if running in jupyter notebook mode\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    !sudo -H apt-get update\n",
    "    !sudo -H apt-get install libtinfo5\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir transformers==4.47.0\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir tokenizers==0.21.0\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir jinja2==3.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow\n",
    "This notebook covers the following\n",
    "1. Parametrizing the Environment\n",
    "2. Instantiate and evaluate FP32 HuggingFace model\n",
    "3. Instantiate and adapt FP32 HuggingFace model\n",
    "4. Model Sample Input\n",
    "5. Prepare model using AIMET model preparer pro\n",
    "6. Evaluation of prepared base model\n",
    "7. Quantization\n",
    "8. Exporting base model onnx, encodings and test vectors\n",
    "\n",
    "### What this notebook is not \n",
    "* This notebook is not intended to show the full scope of optimization. For example, the flow will not use QAT, KD-QAT as deliberate choice to have the notebook execute more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.1 Notebook Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Notebook Features Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "context_length = int(os.getenv(\"CONTEXT_LENGTH\", 3073))\n",
    "\n",
    "enable_right_padding = os.getenv(\"ENABLE_RIGHT_PADDING\", 'True').lower() in ('true', '1', 't')  # right padding of kvcache\n",
    "\n",
    "anchor_alpha = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Notebook Quantization Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_decoder_seqmse = os.getenv(\"APPLY_DECODER_SEQMSE\", 'False').lower() in ('true', '1', 't') \n",
    "\n",
    "apply_lm_head_seqmse = os.getenv(\"APPLY_LM_HEAD_SEQMSE\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "apply_decoder_lpbq = os.getenv(\"APPLY_DECODER_LPBQ\", 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "apply_lm_head_lpbq = os.getenv(\"APPLY_LM_HEAD_LPBQ\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "clamp_val = os.getenv(\"ACTIVATION_CLIPPING_CLAMP_VAL\", 400)\n",
    "\n",
    "embedding_table_bitwidth = int(os.getenv(\"EMBEDDING_TABLE_BITWIDTH\", 8))  # This can be either 8 or 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Notebook Configs that will impact Notebook run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_fp16 = os.getenv(\"ENABLE_FP16\", 'False').lower() in ('true', '1', 't') # Flag to enable e2e fp16 flow, set to false to set fp32 flow\n",
    "\n",
    "run_ppl_eval = os.getenv(\"RUN_PPL_EVAL\", 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "skip_prepare = os.getenv(\"SKIP_PREPARE\", 'False').lower() in ('true', '1', 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert context_length <= 8273, \"Context length longer than 8273 for llama2 model family has not been validated for accuracy\"\n",
    "#assert not (apply_decoder_lpbq and apply_lm_head_lpbq), \"Applying LPBQ to both Decoder and LM-Head has not been validated for accuracy\"\n",
    "assert embedding_table_bitwidth in (8, 16), \"Only 8-bit and 16-bit Emebdding Table have been validated\"\n",
    "assert not enable_fp16, \"FP16 based quantization has not been tested\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2 Setting QNN SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "QNN_SDK_ROOT = os.getenv('QNN_SDK_ROOT', None)\n",
    "assert QNN_SDK_ROOT is not None, 'Please point the QNN_SDK_ROOT variable to your QNN SDK'\n",
    "assert os.path.exists(QNN_SDK_ROOT), \"QNN_SDK_ROOT doesn't exist!\"\n",
    "lib_clang_path = os.path.join(QNN_SDK_ROOT, 'lib', 'x86_64-linux-clang')\n",
    "sys.path.insert(0, QNN_SDK_ROOT + '/lib/python')\n",
    "LD_LIBRARY_PATH = os.getenv('LD_LIBRARY_PATH', None)\n",
    "os.environ['LD_LIBRARY_PATH'] = lib_clang_path + ':' + LD_LIBRARY_PATH if LD_LIBRARY_PATH is not None else lib_clang_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Setting NSP Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from utilities.nsptargets import NspTargets\n",
    "\n",
    "# setup Target platform and its generation\n",
    "TARGET_PLATFORM = os.getenv(\"TARGET_PLATFORM\", \"Windows\").capitalize()\n",
    "\n",
    "# Android GEN4 and GEN5 is supported for this notebook\n",
    "PLATFORM_GEN = int(os.getenv(\"PLATFORM_GEN\", 2))\n",
    "\n",
    "nsp_target = eval(f\"NspTargets.{TARGET_PLATFORM}.GEN{PLATFORM_GEN}\")\n",
    "\n",
    "# Select quantsim config based on target\n",
    "htp_config_file = f'/usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/htp_quantsim_config_{nsp_target.dsp_arch}.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Instantiate and evaluate HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.llama import modeling_llama\n",
    "from aimet_torch.utils import place_model, change_tensor_device_placement\n",
    "from aimet_torch.pro.utils.profiler import event_marker\n",
    "\n",
    "model_name = os.getenv(\"MODEL_NAME\", 'llamav2')\n",
    "\n",
    "model_id = os.getenv(\"MODEL_ID\", None)\n",
    "\n",
    "cache_dir = os.getenv(\"CACHE_DIR\", './cache_dir')\n",
    "\n",
    "output_dir = os.getenv(\"OUTPUT_DIR\", f\"./output_dir\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell (and the corresponding cells with Recipe_logger tag) can be removed after dumping and verifying the recipe without \n",
    "# impacting notebook functionality\n",
    "from genai_lib.common.debug.recipe_logger import recipe_dump_init\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_env_info\n",
    "\n",
    "# Recipe_logger: Initialize the logger and log environment details \n",
    "recipe_dump_init(output_dir)\n",
    "\n",
    "llm_lib_log_env_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1 Configurable setting by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "llm_config = AutoConfig.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "# To help with debugging num_hidden_layers could be set to 2 to quickly verify the pipeline and export a two layer model for verification purposes\n",
    "num_hidden_layers = int(os.getenv(\"NUM_HIDDEN_LAYERS\", 0))\n",
    "llm_config.num_hidden_layers = num_hidden_layers if num_hidden_layers > 0 else llm_config.num_hidden_layers\n",
    "\n",
    "print(f'num_layer: {llm_config.num_hidden_layers}, context_length: {context_length}, '\n",
    "      f'num_hidden_size: {llm_config.num_attention_heads}, num_kv_heads: {llm_config.num_key_value_heads}')\n",
    "\n",
    "with event_marker('HuggingFace FP model creation'):\n",
    "    model = modeling_llama.LlamaForCausalLM.from_pretrained(model_id, config=llm_config)\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = '0'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, use_fast=True, trust_remote_code=True)\n",
    "    # Adjust the tokenizer to limit to context_length\n",
    "    tokenizer.model_max_length = context_length\n",
    "\n",
    "# Reduce the precision of the model to FP16 to minimize the amount of GPU memory needed\n",
    "if enable_fp16:\n",
    "    model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Instantiate Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.wikitext_dataloader import get_wiki_dataset\n",
    "\n",
    "valid_datasets = {}\n",
    "\n",
    "with event_marker(\"Instantiate wikitext Dataloaders\"):\n",
    "    wiki_train_dataloader, wiki_test_dataloader, wiki_dataset = get_wiki_dataset(context_length, tokenizer, cache_dir)\n",
    "\n",
    "valid_datasets[\"WIKITEXT\"] = {\n",
    "    \"dataloader\": wiki_train_dataloader,\n",
    "    \"dataset\": wiki_dataset\n",
    "}\n",
    "\n",
    "base_calibration_key = os.getenv(\"BASE_CALIBRATION_DATASET\", \"WIKITEXT\").upper()\n",
    "\n",
    "assert base_calibration_key in valid_datasets, (\n",
    "   f\"`BASE_CALIBRATION_DATASET` must be one of {list(valid_datasets)}, \"\n",
    "   f\"but got {base_calibration_key}\"\n",
    ")\n",
    "\n",
    "base_calibration_dataloader = valid_datasets[base_calibration_key][\"dataloader\"]\n",
    "print(\"Using base calibration dataset:\", base_calibration_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3 HuggingFace FP model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.llm.evaluation_utils import llm_evaluate_ppl_with_dataloader\n",
    "\n",
    "if run_ppl_eval:\n",
    "    with event_marker(\"HuggingFace FP model eval\"):\n",
    "        with place_model(model, torch.device('cuda')):\n",
    "            orig_ppl = llm_evaluate_ppl_with_dataloader(model=model, dataloader=wiki_test_dataloader)\n",
    "\n",
    "    print(f\"PPL score of HuggingFace FP model = {orig_ppl}\")\n",
    "\n",
    "# Remove the HuggingFace model from memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_property, Property\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_metric, ModelType, Metric\n",
    "\n",
    "# Recipe_logger: Log the context_length property and the metrics.\n",
    "llm_lib_log_property({Property.context_length : context_length})\n",
    "\n",
    "if run_ppl_eval:\n",
    "    llm_lib_log_metric(ModelType.hf_model, Metric.ppl, orig_ppl, model_name=\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Instantiate and adapt FP32 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1 Adapt FP32 model definition for inference on HTP.\n",
    "- The following adaptations are done to replace default attention module with attention definition that compatible with NSP backend\n",
    "  * use conv instead of linear for Q,K,V,O projections\n",
    "  * bypass attention and causal mask generation and replace with pre-generated 2D-mask input\n",
    "  * output only newly created V and transposed K instead of entire augmented KV sequence\n",
    "  * input pre-calculated positional embedding instead of position ids, thus bypass the embedding generation in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.llama import modeling_llama\n",
    "from transformers import cache_utils\n",
    "from aimet_torch.pro.utils.profiler import event_marker\n",
    "from genai_lib.llm.dev.model_adaptation.llama.adaptation import (\n",
    "    QcLlamaAttention,\n",
    "    adapted_update_causal_mask,\n",
    "    adapted_RotaryEmbedding,\n",
    "    DynamicCache_update,\n",
    "    DynamicCache_get_seq_length,\n",
    "    update_attr,\n",
    "    QcLlamaForCausalLM,\n",
    "    DynamicCache_to_legacy_cache,\n",
    ")\n",
    "\n",
    "with event_marker(\"FP model adaptation configuration\"):\n",
    "    modeling_llama.LLAMA_ATTENTION_CLASSES['eager'] = QcLlamaAttention\n",
    "    modeling_llama.LlamaForCausalLM = QcLlamaForCausalLM\n",
    "\n",
    "    # Bypass attention_mask preparation\n",
    "    assert hasattr(modeling_llama.LlamaModel, '_update_causal_mask'), \\\n",
    "    \"LLamaModel does not have _update_causal_mask as attribute\"\n",
    "    modeling_llama.LlamaModel._update_causal_mask = adapted_update_causal_mask\n",
    "\n",
    "    # Bypass rotary_emb module\n",
    "    assert hasattr(modeling_llama.LlamaRotaryEmbedding, 'forward'), \\\n",
    "    f\"Unknown LlamaRotaryEmbedding definition: {modeling_llama.LlamaRotaryEmbedding}\"\n",
    "    modeling_llama.LlamaRotaryEmbedding.forward = adapted_RotaryEmbedding\n",
    "\n",
    "    # Adapting KV$ management\n",
    "    assert update_attr(cache_utils.DynamicCache, 'update', DynamicCache_update), f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\"\n",
    "    assert update_attr(cache_utils.DynamicCache, 'get_seq_length', DynamicCache_get_seq_length),  f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\"\n",
    "    assert update_attr(cache_utils.DynamicCache, 'to_legacy_cache', DynamicCache_to_legacy_cache), \\\n",
    "    f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Instantiate adapted FP32 model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================Fixed setting that should not be changed by users==============\n",
    "# Auto-regression length: number of tokens to consume and number of logits to produce.\n",
    "# This value should NOT be changed due to downstream consumption requirements\n",
    "ARN = int(os.getenv(\"ARN\", 2073))\n",
    "\n",
    "enable_right_padding =  enable_right_padding # enable_right_padding is always True when enabled long context length\n",
    "\n",
    "pad_to_left = not enable_right_padding\n",
    "\n",
    "setattr(llm_config, 'return_new_key_value_only', True)\n",
    "setattr(llm_config, 'transposed_key_cache', True)\n",
    "setattr(llm_config, 'use_combined_mask_input', True)\n",
    "setattr(llm_config, 'use_position_embedding_input', True)\n",
    "setattr(llm_config, '_attn_implementation', 'eager')\n",
    "setattr(llm_config, '_attn_implementation_internal', 'eager')\n",
    "setattr(llm_config, 'return_dict', False)\n",
    "setattr(llm_config, 'num_logits_to_keep', 0)\n",
    "setattr(llm_config, 'input_tokens_per_inference', ARN)\n",
    "    \n",
    "num_slices=(context_length/8 + ARN - 1)//ARN\n",
    "\n",
    "llm_config.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_property, Property\n",
    "\n",
    "# Recipe_logger: Log the ARN of the prepared model\n",
    "llm_lib_log_property({Property.ARN : ARN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with event_marker('Adapted FP model creation'):\n",
    "    model = modeling_llama.LlamaForCausalLM.from_pretrained(model_id, config=llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.3 Changes to HuggingFace model to work with the Adapted Model or Prepared Model\n",
    "- As a result of adapting the model we introduce changes to the types of the model inputs.\n",
    "- As a result of model preparation, we make the shapes of the inputs static.\n",
    "- adapted_model_forward works with either adapted model dynamic input or prepared model static input model through flag static_shape.\n",
    "- Override the 'forward' function and the function 'prepare_inputs_for_generation'. With these overrides, we make the adapted model or prepared model work just like the old model.\n",
    "- adapted_model_prepare_inputs_for_dynamic_shapes is utility function for forward pass of adapted model with dynamic shapes.\n",
    "- adapted_model_prepare_inputs_for_static_shapes is utility function for forward pass of prepared model with static shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.llm.static_graph_utils import llm_pad_inputs, llm_create_1d_attn_mask, llm_pad_past_kv, \\\n",
    "    llm_get_position_ids_from_attention_mask, llm_pad_input_attn_mask, llm_create_kv_attn_mask, llm_get_dummy_kv,\\\n",
    "    llm_trim_pad_logits, llm_pad_position_ids,llm_slice_inputs_for_inference\n",
    "from genai_lib.llm.dev.model_adaptation.llama.utils import llm_update_causal_mask, llm_create_position_embeddings\n",
    "from genai_lib.llm.dev.model_adaptation.common.utils import KEY_CONCAT_AXIS, VALUE_CONCAT_AXIS, llm_update_kv_cache\n",
    "from genai_lib.llm.long_context_utils import llm_compute_scores, llm_scatter_exceeded_kv_using_lazy_eviction, llm_update_overwriting_cache\n",
    "from genai_lib.common.dev.utils import change_signature_defaults\n",
    "from aimet_torch.utils import change_tensor_device_placement\n",
    "import types\n",
    "import random\n",
    "\n",
    "\n",
    "def adapted_model_prepare_inputs_for_dynamic_shapes(self,input_ids_slice, attn_mask_slice, position_ids_slice, outputs, **kwargs):\n",
    "    device = input_ids_slice.device\n",
    "    batch_size = input_ids_slice.shape[0]\n",
    "    pad_token = tokenizer.eos_token_id\n",
    "    head_dim = llm_config.head_dim if hasattr(llm_config, 'head_dim') else llm_config.hidden_size // llm_config.num_attention_heads\n",
    "\n",
    "    kv_length=0\n",
    "    if outputs['past_key_values'] is None:\n",
    "        kv_length = 0\n",
    "    elif not isinstance(outputs['past_key_values'], tuple):\n",
    "        kv_length = outputs['past_key_values'].get_seq_length()\n",
    "    else:\n",
    "        kv_length = outputs['past_key_values'][0][1].shape[-2]    \n",
    "\n",
    "    if pad_to_left:\n",
    "        cache_index = None\n",
    "    else:\n",
    "        cache_index = torch.tensor([kv_length], dtype=torch.int64, device=device)\n",
    "\n",
    "    past_kv_attn_mask = torch.ones((batch_size, kv_length), dtype=torch.long, device=device)\n",
    "    prepared_1d_attention_mask = llm_create_1d_attn_mask(attn_mask_past_kv=past_kv_attn_mask,\n",
    "                                                         attn_mask_input=attn_mask_slice,\n",
    "                                                         cache_index=cache_index)\n",
    "\n",
    "\n",
    "    prepared_causal_mask = llm_update_causal_mask(prepared_1d_attn_mask=prepared_1d_attention_mask,\n",
    "                                                  input_tensor=input_ids_slice,\n",
    "                                                  max_input_tokens= input_ids_slice.shape[-1],\n",
    "                                                  model_context_len=context_length,\n",
    "                                                  model_id_or_path=model_id,\n",
    "                                                  cache_index=cache_index,\n",
    "                                                  pad_to_left = pad_to_left)\n",
    "\n",
    "    ########### Position ID preparation #######\n",
    "\n",
    "    padded_position_ids = llm_pad_position_ids(position_ids_slice=position_ids_slice,\n",
    "                                                max_input_tokens=ARN, \n",
    "                                                pad_to_left = pad_to_left)\n",
    "    prepared_position_embeddings = llm_create_position_embeddings(config = llm_config,\n",
    "                                                                  position_ids = padded_position_ids)\n",
    "\n",
    "\n",
    "\n",
    "    prepared_inputs = {\n",
    "        'input_ids': input_ids_slice,\n",
    "        'attention_mask': prepared_causal_mask,\n",
    "        'position_ids': prepared_position_embeddings,\n",
    "        'past_key_values': outputs['past_key_values'],\n",
    "    }\n",
    "\n",
    "    if enable_right_padding:\n",
    "        prepared_inputs.update({'cache_index': cache_index})\n",
    "\n",
    "    return prepared_inputs\n",
    "\n",
    "\n",
    "def adapted_model_prepare_inputs_for_static_shapes(self,input_ids_slice, attn_mask_slice, position_ids_slice, outputs):\n",
    "    batch_size = input_ids_slice.shape[0]\n",
    "    pad_token = tokenizer.eos_token_id\n",
    "    device = input_ids_slice.device\n",
    "    head_dim = llm_config.head_dim if hasattr(llm_config, 'head_dim') else llm_config.hidden_size // llm_config.num_attention_heads\n",
    "\n",
    "    ####### input id preparation #######\n",
    "    pad_input_ids = llm_pad_inputs(pad_token=pad_token,\n",
    "                                   max_input_tokens=ARN,\n",
    "                                   input_ids_slice=input_ids_slice,\n",
    "                                   pad_to_left=pad_to_left)\n",
    "\n",
    "    ####### KV input preparation #######\n",
    "    dummy_kv = llm_get_dummy_kv(batch_size=batch_size,\n",
    "                                num_key_value_heads=llm_config.num_key_value_heads,\n",
    "                                head_dim= head_dim,\n",
    "                                key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                device=device,\n",
    "                                cache_len=context_length-ARN if pad_to_left else context_length)\n",
    "\n",
    "    padded_past_kv_in = llm_pad_past_kv(dummy_past_kv=dummy_kv,\n",
    "                                        unpadded_past_kv=outputs['past_key_values'],\n",
    "                                        num_hidden_layers = llm_config.num_hidden_layers,\n",
    "                                        key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                        value_concat_axis=VALUE_CONCAT_AXIS,\n",
    "                                        pad_to_left=pad_to_left)\n",
    "\n",
    "\n",
    "    ######### Attention mask Input preparation #######\n",
    "    inp_attn_mask = llm_pad_input_attn_mask(attn_mask_slice=attn_mask_slice,\n",
    "                                            max_input_tokens=ARN,\n",
    "                                            pad_to_left=pad_to_left)\n",
    "\n",
    "    kv_length = 0\n",
    "    if outputs['past_key_values'] is None:\n",
    "        kv_length = 0\n",
    "    elif not isinstance(outputs['past_key_values'], tuple):\n",
    "        kv_length = outputs['past_key_values'].get_seq_length()\n",
    "    else:\n",
    "        kv_length = outputs['past_key_values'][0][1].shape[-2]\n",
    "    \n",
    "    past_kv_attn_mask = llm_create_kv_attn_mask(unpadded_past_kv= outputs['past_key_values'],\n",
    "                                                model_context_len=context_length,\n",
    "                                                max_input_tokens=ARN,\n",
    "                                                batch_size=batch_size,\n",
    "                                                device=device,\n",
    "                                                pad_to_left=pad_to_left)\n",
    "\n",
    "    if pad_to_left:\n",
    "        cache_index = None\n",
    "    else:\n",
    "        cache_index = torch.tensor([kv_length], dtype=torch.int64, device=device)\n",
    "\n",
    "    prepared_1d_attention_mask = llm_create_1d_attn_mask(attn_mask_past_kv=past_kv_attn_mask,\n",
    "                                                         attn_mask_input=inp_attn_mask,\n",
    "                                                         cache_index=cache_index)\n",
    "\n",
    "    # due to model adaptation\n",
    "    prepared_causal_mask = llm_update_causal_mask(prepared_1d_attn_mask = prepared_1d_attention_mask,\n",
    "                                                  input_tensor = pad_input_ids,\n",
    "                                                  max_input_tokens = ARN,\n",
    "                                                  model_context_len = context_length,\n",
    "                                                  model_id_or_path = model_id,\n",
    "                                                  cache_index=cache_index,\n",
    "                                                  pad_to_left= pad_to_left)\n",
    "\n",
    "    ########### Position ID preparation #######\n",
    "    padded_position_ids = llm_pad_position_ids(position_ids_slice=position_ids_slice,\n",
    "                                                max_input_tokens=ARN, \n",
    "                                                pad_to_left = pad_to_left)\n",
    "    # model adaptation\n",
    "    prepared_position_embeddings = llm_create_position_embeddings(config = llm_config,\n",
    "                                                                  position_ids = padded_position_ids)\n",
    "\n",
    "\n",
    "    prepared_inputs = {\n",
    "        'input_ids': pad_input_ids,\n",
    "        'attention_mask': prepared_causal_mask,\n",
    "        'position_ids': prepared_position_embeddings,\n",
    "        'past_key_values':padded_past_kv_in\n",
    "    }\n",
    "\n",
    "    if enable_right_padding:\n",
    "        prepared_inputs.update({'cache_index': cache_index})\n",
    "\n",
    "    return prepared_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "# Redefinition of the forward function to work with model I/O adaptations and static shapes of the tensors that the model consumes as input\n",
    "def adapted_model_forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    past_key_values=None,\n",
    "    inputs_embeds=None,\n",
    "    return_dict=False,\n",
    "    output_hidden_states=False,\n",
    "    **kwargs\n",
    "):\n",
    "    kv_length = 0 if past_key_values is None else past_key_values.get_seq_length() if not isinstance(past_key_values, tuple) else past_key_values[0][1].shape[-2]\n",
    "    if kv_length == 0:\n",
    "        self.initial_prompt_length = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "        self.tokens_seen_so_far = 0\n",
    "            \n",
    "    position_ids = None\n",
    "    device = input_ids.device\n",
    "    static_shape = hasattr(self, 'num_logits_to_return')\n",
    "    num_slices = kwargs.get('num_slices', None)\n",
    "    if hasattr(self, 'tokens_seen_so_far'):\n",
    "        position_ids = torch.arange(self.tokens_seen_so_far, self.tokens_seen_so_far + input_ids.shape[1]).unsqueeze(\n",
    "            0).repeat(input_ids.shape[0], 1).to(input_ids.device)\n",
    "        self.tokens_seen_so_far += input_ids.shape[1]\n",
    "    \n",
    "    # create the generator which slices input into chunks of AR (and pads if necessary)\n",
    "    slice_inputs_gen_obj = llm_slice_inputs_for_inference(max_input_tokens=ARN if static_shape else input_ids.shape[-1],\n",
    "                                                          model_context_len=context_length,\n",
    "                                                          input_ids=input_ids,\n",
    "                                                          position_ids=position_ids)\n",
    "    # dictionary to store the running output which contains the logits and the useful past kv cache until that execution\n",
    "    outputs = {}\n",
    "    outputs['past_key_values'] = past_key_values\n",
    "    for i, inputs in enumerate(slice_inputs_gen_obj):\n",
    "        input_ids_slice = inputs['input_ids_slice']\n",
    "        attn_mask_slice = inputs['attn_mask_slice']\n",
    "        position_ids_slice = inputs['position_ids_slice']\n",
    "        if num_slices is not None and i >= num_slices:\n",
    "            break\n",
    "        kv_length = 0 if outputs['past_key_values'] is None else outputs['past_key_values'].get_seq_length() if not isinstance(outputs['past_key_values'], tuple) else outputs['past_key_values'][0][1].shape[-2]\n",
    "\n",
    "        if static_shape:  \n",
    "            prepared_inputs = adapted_model_prepare_inputs_for_static_shapes(self,input_ids_slice=input_ids_slice,\n",
    "                                                                             attn_mask_slice=attn_mask_slice, position_ids_slice=position_ids_slice,\n",
    "                                                                             outputs=outputs)\n",
    "        else:\n",
    "            prepared_inputs = adapted_model_prepare_inputs_for_dynamic_shapes(self, input_ids_slice=input_ids_slice,\n",
    "                                                                              attn_mask_slice=attn_mask_slice, position_ids_slice=position_ids_slice,\n",
    "                                                                              outputs=outputs)\n",
    "\n",
    "        cur_outputs = self.model(**prepared_inputs)\n",
    "        if not static_shape:\n",
    "            cur_outputs = (self.lm_head(cur_outputs[0]),) + cur_outputs[1:]\n",
    "\n",
    "        # the following condition checks whether the size of the KV after accumulation would exceed the budget.\n",
    "        # 1. Compute scores from accumulated_past_kv & the output anchor,\n",
    "        # 2. Update the overwriting_index_cache, \n",
    "        # 3. Concat accumulated kvcache and new kvcache\n",
    "        # 4. Scatter the new KV into positions of old KV\n",
    "        # If running SSD, make sure to adjust this computation to account for the prefix kv in the budget.\n",
    "        # avoided creating a new tuple of current_key_value to avoid the memory spike, sending slice\n",
    "        num_exceed_kv = kv_length + input_ids_slice.shape[-1] - (context_length -ARN)\n",
    "        overwrite_cache = False\n",
    "        if hasattr(self, \"overwriting_index_cache\") and self.overwriting_index_cache is not None:  \n",
    "            # Update the overwriting_index_cache if necessary            \n",
    "            if self.overwriting_index_cache.get(0,None) is None:\n",
    "                overwrite_cache = True\n",
    "            elif self.overwriting_index_cache.get(0,None).shape[-2] < num_exceed_kv:\n",
    "                overwrite_cache = True\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        outputs['past_key_values'] = llm_update_kv_cache(unpadded_past_kv = outputs['past_key_values'],\n",
    "                                                         current_key_values= cur_outputs[1],\n",
    "                                                         key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                                         value_concat_axis=VALUE_CONCAT_AXIS,\n",
    "                                                         input_ids_slice = input_ids_slice,\n",
    "                                                         pad_to_left=pad_to_left)\n",
    "\n",
    "        lm_logits = llm_trim_pad_logits(cur_logits = cur_outputs[0],\n",
    "                                        input_ids_slice=input_ids_slice,\n",
    "                                        pad_to_left=pad_to_left)\n",
    "        bsz, _, dim = lm_logits.shape\n",
    "        outputs['logits'] = torch.cat(\n",
    "                (outputs.get('logits', torch.zeros((bsz, 0, dim), device=lm_logits.device)), lm_logits),\n",
    "                dim=1)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            last_hidden_states = llm_trim_pad_logits(cur_logits = cur_outputs[2][-1],\n",
    "                                                     input_ids_slice=input_ids_slice)\n",
    "            bsz, _, dim = last_hidden_states.shape\n",
    "            outputs['hidden_states'] = torch.cat(\n",
    "                    (outputs.get('hidden_states', torch.zeros((bsz, 0, dim), device=last_hidden_states.device)), last_hidden_states),\n",
    "                    dim=1)\n",
    "\n",
    "    if return_dict:\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=outputs.get('loss', None),\n",
    "            logits=outputs.get('logits', None),\n",
    "            past_key_values=outputs.get('past_key_values', None),\n",
    "            hidden_states=outputs.get('hidden_states', None),\n",
    "            attentions=outputs.get('attentions', None),\n",
    "        )\n",
    "    return tuple(outputs.get(out) for out in ['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'] if outputs.get(out) is not None)\n",
    "\n",
    "def adapted_model_prepare_inputs_for_generation(\n",
    "            self, input_ids=None, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "    # \"past_key_values is None\" indicates that `prepare_inputs_for_generation()` is called first time in `generate()` when model_mode is kvcache\n",
    "    # In first inference, we should pass all inputs to get valid kv cache\n",
    "\n",
    "    # the self.tokens_seen_so_far tells the model how many tokens the model has seen so far, if present the attribute stores the running sum of tokens fed so far.\n",
    "    # This is particularly useful during generation in LC when we want to infer the position ids of the newly generated tokens during generation\n",
    "    # (position_ids have historically been inferred from the attention mask but it no longer holds true for LC)\n",
    "\n",
    "    # self.initial_prompt_length stores the total length of the prompt passed for inference. This is used to distinguish between the prefill and decode stage in the adapted forward pass.\n",
    "    # If the forward pass has seen fewer tokens than the prompt length, it is the prefill stage, else decode stage.\n",
    "\n",
    "    kv_length = past_key_values.get_seq_length() if not isinstance(past_key_values, tuple) else past_key_values[0][1].shape[-2]\n",
    "    if kv_length == 0:\n",
    "        self.initial_prompt_length = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "        self.tokens_seen_so_far = 0\n",
    "        self.overwriting_index_cache = None\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        return {\n",
    "                \"input_ids\": input_ids[:, self.tokens_seen_so_far:],\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"attention_mask\": attention_mask\n",
    "            }\n",
    "\n",
    "    else:\n",
    "        generated_embeddings = self.model.embed_tokens(input_ids[-1])\n",
    "        inputs_embeds = torch.cat(\n",
    "            (inputs_embeds, torch.unsqueeze(generated_embeddings, dim=0).to(device=inputs_embeds.device)), dim=1)\n",
    "        return {\n",
    "            \"input_ids\": None,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"inputs_embeds\": inputs_embeds[:, kv_length:, :]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.4 Complete the last step(s) of Model Adaptation\n",
    "The following model adaptation are enabled for inference:\n",
    "- apply linear to conv in attention, MLP and lmhead and arrange linear weights properly for conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.common.dev.model_adaptation.linear_to_conv import ConvInplaceLinear, replace_linears_with_convs\n",
    "\n",
    "with event_marker('FP model adaptation for NSP backend completion'):\n",
    "    model = replace_linears_with_convs(model)\n",
    "\n",
    "if run_ppl_eval:\n",
    "    model.forward = types.MethodType(adapted_model_forward, model)\n",
    "    with event_marker(f\"Adapted FP model eval\"):\n",
    "        with place_model(model, torch.device('cuda')):\n",
    "            adapted_ppl = llm_evaluate_ppl_with_dataloader(model=model, dataloader=wiki_test_dataloader)\n",
    "    print(f\"PPL score of Adapted HF FP model = {adapted_ppl}\")\n",
    "\n",
    "    # Revert forward passes for model preparation\n",
    "    model.forward = types.MethodType(QcLlamaForCausalLM.forward, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    llm_lib_log_metric(ModelType.adapted_model, Metric.ppl, adapted_ppl, model_name=\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_data(device=\"cuda\", dtype=torch.float32, return_dict=False):\n",
    "    input_ids = torch.randint(0, len(tokenizer), (1, ARN), device=device)\n",
    "    attn_mask = torch.ones((1, ARN), device=device, dtype=dtype)\n",
    "    position_ids = torch.randint(0, len(tokenizer), (1, ARN), device=device) #1,ARN\n",
    "    outputs={}\n",
    "    outputs['past_key_values']=None\n",
    "    dummy_input = adapted_model_prepare_inputs_for_static_shapes(model, input_ids, attn_mask, position_ids, outputs)\n",
    "    for val in dummy_input:\n",
    "        dummy_input[val]= change_tensor_device_placement(dummy_input[val], device)\n",
    "    if not return_dict:\n",
    "        dummy_input = tuple(dummy_input.values())\n",
    "    return dummy_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare model using AIMET model preparer pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.1 KVCache MHA model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix LazyQuantizeWrapper attribute delegation\n",
    "Monkey patch in AIMET to fix exception rule failure with nonleaf qmodules, it ensures attributes not found in the wrapper are properly delegated to the wrapped module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.quantsim_config.builder import LazyQuantizeWrapper\n",
    "\n",
    "original_LazyQuantizeWrapper = LazyQuantizeWrapper\n",
    "\n",
    "class FixedLazyQuantizeWrapper(original_LazyQuantizeWrapper):\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self._module_to_wrap, name)\n",
    "\n",
    "LazyQuantizeWrapper = FixedLazyQuantizeWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from aimet_torch.utils import load_pytorch_model\n",
    "import aimet_torch.pro.ir_graph_op_handler as ir_graph_op_handler\n",
    "from aimet_torch import onnx_utils\n",
    "from aimet_torch.pro import model_preparer\n",
    "from genai_lib.llm.model_preparation_utils import llm_build_preparer_converter_args\n",
    "from genai_lib.llm.utils import llm_model_input_output_names\n",
    "\n",
    "# Setting this flag to False means that the prepared model will be flattened\n",
    "onnx_utils.EXPORT_TO_ONNX_DIRECT = True\n",
    "\n",
    "# This flag must be set to false because we rely on the model structure being flat to enable weight sharing\n",
    "ir_graph_op_handler.KEEP_ORIGINAL_MODEL_STRUCTURE = False\n",
    "\n",
    "model.num_logits_to_return = ARN # configuring the model for KVCache mode\n",
    "\n",
    "prepare_path = os.path.join(output_dir, 'prepare')\n",
    "os.makedirs(prepare_path, exist_ok=True)\n",
    "prepare_filename = f'{model_name}_kvcache_{llm_config.num_hidden_layers}_layer'\n",
    "\n",
    "if skip_prepare:\n",
    "    with event_marker(f\"KVCache load pre-prepared {prepare_filename}\", flush_ram=True):\n",
    "        prepared_model_path = os.path.join(prepare_path, f'{prepare_filename}.py')\n",
    "        if not os.path.exists(prepared_model_path):\n",
    "            raise ValueError(f\"prepared artifacts not found in {prepare_path}\")\n",
    "        else:\n",
    "            print(f'WARNING: preparation skipped for model={prepare_filename}, prepared at {time.ctime(os.path.getmtime(prepared_model_path))}')\n",
    "            prepared_model = load_pytorch_model(path=prepare_path, filename=prepare_filename,\n",
    "                                                model_name=prepare_filename, load_state_dict=True)\n",
    "\n",
    "else:\n",
    "    dummy_input = get_dummy_data(device=model.model.device, dtype=model.dtype, return_dict=True)\n",
    "    input_names, output_names = llm_model_input_output_names(llm_config.num_hidden_layers)\n",
    "    if enable_right_padding:\n",
    "        input_names += [\"cache_index\"]\n",
    "        \n",
    "    # Build converter args\n",
    "    converter_args = llm_build_preparer_converter_args(llm_config.num_hidden_layers, input_names)\n",
    "    with event_marker(\"KVCache prepare model\", flush_ram=True):\n",
    "        model_preparer.ORDER_INPUTS = True\n",
    "        model_preparer.ORDER_OUTPUTS = True\n",
    "        if __name__ == '__main__': # We use the main guard to prevent child processes from re-running the top-level code\n",
    "            prepared_model = model_preparer.prepare_model(model,\n",
    "                                                          dummy_input,\n",
    "                                                          model_name=prepare_filename,\n",
    "                                                          filename=prepare_filename,\n",
    "                                                          path=prepare_path,\n",
    "                                                          input_names=input_names,\n",
    "                                                          output_names=output_names,\n",
    "                                                          onnx_export_args={\"opset_version\":17},\n",
    "                                                          converter_args=converter_args,\n",
    "                                                          skipped_optimizers=['eliminate_common_subexpression',\n",
    "                                                                              'eliminate_nop_with_unit', \n",
    "                                                                              'eliminate_duplicate_initializer'\n",
    "                                                                             ]\n",
    "                                                          )\n",
    "        else:\n",
    "            raise Exception(\"Killing multiprocessing spawn started by Converter during model preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation of prepared model\n",
    "Verify if prepared KV cache model generates the same PPL as FP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.1 Changes to HuggingFace model to work with the prepared model\n",
    "\n",
    "Replace the model inside the HuggingFace model with the prepared model.\n",
    "Note that the prepared model already fuses model.model and model.lm_head \n",
    "into one, so here we simply set model.lm_head to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.model\n",
    "del model.lm_head\n",
    "\n",
    "model.model = None\n",
    "model.lm_head = None\n",
    "\n",
    "model.forward = types.MethodType(adapted_model_forward, model)\n",
    "model.prepare_inputs_for_generation = types.MethodType(adapted_model_prepare_inputs_for_generation, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2 Convert the model to half precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_fp16:\n",
    "    torch.set_default_dtype(torch.float16)\n",
    "    model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.3 Evaluation of perplexity score using prepared model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    with event_marker(\"KVcache prepared FP eval\", flush_ram=True):\n",
    "        with place_model(prepared_model, torch.device(\"cuda\")):\n",
    "            model.model = prepared_model\n",
    "            prepared_kvcache_ppl = llm_evaluate_ppl_with_dataloader(model=model, dataloader=wiki_test_dataloader)\n",
    "\n",
    "    # This should be very close (<1e-4 delta) to original model's perplexity\n",
    "    # If the perplexity score goes further up, it indicates the AIMET/QNN pair is producing a faulty prepared model\n",
    "    print(f\"ppl score of KVCACHE prepared fp model = {prepared_kvcache_ppl}\")\n",
    "    print(f\"Diff between HF orig ppl and prepared ppl = {orig_ppl - prepared_kvcache_ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    llm_lib_log_metric(ModelType.prepared_model, Metric.ppl, prepared_kvcache_ppl, model_name=\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Quantization\n",
    "\n",
    "The _Quantization_ step is the primary focus of this notebook, this section could be modified to execute various quantization experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.1 Create quantsim configured for QNN HTP target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v2.quantsim import QuantizationSimModel\n",
    "import aimet_common.quantsim as qs\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "\n",
    "qs.encoding_version = '1.0.0'\n",
    "\n",
    "if apply_lm_head_seqmse or apply_decoder_seqmse:\n",
    "    import functools\n",
    "\n",
    "    def copy_model_with_shared_weights(source_model):\n",
    "        target_model = deepcopy(source_model)\n",
    "        for name, source_parameter in source_model.named_parameters():\n",
    "            pre, _, post = name.rpartition('.')\n",
    "            pre_obj = functools.reduce(getattr, [target_model] + pre.split('.')) if pre else target_model\n",
    "            setattr(pre_obj, post, source_parameter)\n",
    "        return target_model\n",
    "\n",
    "    # Create copy of fp model defintion for SeqMSE and/or LoRA\n",
    "    fp_prepared_model = copy_model_with_shared_weights(prepared_model)\n",
    "\n",
    "dummy_input = get_dummy_data(device = \"cuda\", dtype = model.dtype, return_dict = True)\n",
    "\n",
    "sig = inspect.signature(prepared_model.forward)\n",
    "dummy_input_sorted = {}\n",
    "for key in list(sig.parameters.keys()):\n",
    "    dummy_input_sorted[key] = dummy_input[key]\n",
    "dummy_input = tuple(dummy_input_sorted.values())\n",
    "\n",
    "with event_marker(\"create KVCache Quantsim\"):\n",
    "    with place_model(prepared_model, \"cuda\"):\n",
    "        quantsim = QuantizationSimModel(model=prepared_model,\n",
    "                                        quant_scheme=QuantScheme.post_training_tf,\n",
    "                                        dummy_input=dummy_input,\n",
    "                                        default_output_bw=16,\n",
    "                                        default_param_bw=4,\n",
    "                                        in_place=True,\n",
    "                                        config_file=htp_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.2 Setting 16bit x 8bit matmuls\n",
    "To keep key and value tensors as 8 bits, reducing data I/O costs associated with KV-cache orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.experimental.quantsim_utils import set_matmul_second_input_producer_to_8bit_symmetric\n",
    "\n",
    "set_matmul_second_input_producer_to_8bit_symmetric(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.3 Concat encoding unification\n",
    "configuring concat ops to have shared encoding on input and output activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.experimental import propagate_output_encodings\n",
    "from aimet_torch.nn.modules import custom as aimet_ops\n",
    "\n",
    "propagate_output_encodings(quantsim, aimet_ops.Concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.4 Manual Mixed Precision\n",
    "applying mixed precision configuration to ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./config/mixed_precision_config/llama2_lpbq_gateProjClip.json\", \"r\") as f_in:\n",
    "    mixed_precision_config = json.load(f_in)\n",
    "\n",
    "# Customize mixed precision config based on user parameters\n",
    "for entry in mixed_precision_config['name_list']:\n",
    "    if \"model_embed_tokens_Gather\" in entry['module_name']:\n",
    "        entry['exceptions']['param_exceptions']['bitwidth'] = embedding_table_bitwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.mixed_precision_overrides import ManualQuantsimMixedPrecisionConfig\n",
    "\n",
    "quantsim_adjuster = ManualQuantsimMixedPrecisionConfig(mixed_precision_config_file = mixed_precision_config)\n",
    "quantsim_adjuster.apply_exceptions(quantsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.nn.modules.custom import QuantizedRmsNorm\n",
    "from aimet_torch.v2.quantization.affine import QuantizeDequantize\n",
    "\n",
    "# Make RMSNorm encodings per-tensor (they default to per-channel)\n",
    "for name, qmodule in quantsim.named_qmodules():\n",
    "    if isinstance(qmodule, QuantizedRmsNorm):\n",
    "        qmodule.param_quantizers['weight'] = QuantizeDequantize(shape=(), bitwidth=16, symmetric=False).to(qmodule.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.5 Apply Block Quantization\n",
    "Swapping needed modules' weight quantizers to LPBQ quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.nn.true_quant import QuantizedConv2d\n",
    "from aimet_torch.v2.quantsim.config_utils import set_grouped_blockwise_quantization_for_weights\n",
    "\n",
    "arg = None\n",
    "\n",
    "if apply_decoder_lpbq and apply_lm_head_lpbq:\n",
    "    arg = lambda module: isinstance(module, QuantizedConv2d)\n",
    "elif apply_decoder_lpbq:\n",
    "    arg = lambda module: isinstance(module, QuantizedConv2d) and module.param_quantizers['weight'].bitwidth == 4\n",
    "elif apply_lm_head_lpbq:\n",
    "    lm_head_modules = [qmodule for name, qmodule in quantsim.named_qmodules() if \"lm_head\" in name]\n",
    "    arg = lambda module: module in lm_head_modules and isinstance(module, QuantizedConv2d)\n",
    "    \n",
    "if arg:\n",
    "    BLOCK_QUANT_SIZE = 64\n",
    "    set_grouped_blockwise_quantization_for_weights(sim = quantsim,\n",
    "                                                   arg = arg,\n",
    "                                                   bitwidth = 4,\n",
    "                                                   symmetric = True,\n",
    "                                                   decompressed_bw = 8,\n",
    "                                                   block_size = BLOCK_QUANT_SIZE,\n",
    "                                                   block_grouping = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unify past_key/value_{x}_out encodings (input[2], input[0], output[0]) to upstream Ops  (self_attn_Concat_1/self_attn_v_proj_Conv)\n",
    "\n",
    "def unify_scatter_elements_encodings(source_name, destination_name):\n",
    "\n",
    "    def _find_module_dict(name):\n",
    "        for module_name, module in quantsim.model.named_modules():\n",
    "            if module_name.endswith(name):\n",
    "                start = module_name.find(name)\n",
    "                yield module_name[:start], module\n",
    "\n",
    "    sources = { name:module for name, module in _find_module_dict(source_name) }\n",
    "    destinations = { name:module for name, module in _find_module_dict(destination_name) }\n",
    "\n",
    "    assert len(sources)==len(destinations) and len(sources)> 0, f\"Cannot execute encoding alignment due to mismatched pairing of \\\n",
    "    source and destination quantizers. String matching found {len(sources)} sources, and {len(destinations)} destinations.\"\n",
    "    # copying quantizers from source module \n",
    "    for module_name, source_module in sources.items():\n",
    "        desination_module = destinations[module_name]\n",
    "        desination_module.input_quantizers[2]=source_module.output_quantizers[0]\n",
    "        desination_module.input_quantizers[0]=source_module.output_quantizers[0]\n",
    "        desination_module.output_quantizers[0]=source_module.output_quantizers[0]        \n",
    "\n",
    "if enable_right_padding:\n",
    "    unify_scatter_elements_encodings('self_attn_Concat_1', 'self_attn_ScatterElements_1')\n",
    "    unify_scatter_elements_encodings('self_attn_v_proj_Conv', 'self_attn_ScatterElements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.7 Sequential MSE\n",
    "applying sequential MSE technique to optimize parameter encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _seq_mse_forward_fn(_model, inputs):\n",
    "    model.model = _model\n",
    "    model(**inputs, num_slices=num_slices)\n",
    "\n",
    "if apply_decoder_seqmse or apply_lm_head_seqmse:\n",
    "    from aimet_torch.v2.seq_mse import apply_seq_mse, SeqMseParams\n",
    "\n",
    "    lm_head_fp_modules = [ module for module_name, module in fp_prepared_model.named_modules() if isinstance(module, torch.nn.Conv2d) and 'lm_head' in module_name ]    \n",
    "    decoder_fp_modules = [ module for module_name, module in fp_prepared_model.named_modules() if isinstance(module, torch.nn.Conv2d) and 'lm_head' not in module_name ]\n",
    "\n",
    "    if apply_decoder_seqmse and apply_lm_head_seqmse:\n",
    "        modules_to_exclude = []\n",
    "    elif apply_decoder_seqmse:\n",
    "        modules_to_exclude = lm_head_fp_modules\n",
    "    elif apply_lm_head_seqmse:\n",
    "        modules_to_exclude = decoder_fp_modules\n",
    "\n",
    "    params = SeqMseParams(num_batches=20,\n",
    "                          inp_symmetry='symqt',\n",
    "                          num_candidates=20,\n",
    "                          loss_fn='mse',\n",
    "                          forward_fn = _seq_mse_forward_fn)\n",
    "\n",
    "    with event_marker(\"SeqMSE\"):\n",
    "        with place_model(quantsim.model, torch.device(\"cuda\")), place_model(fp_prepared_model, torch.device(\"cuda\")):\n",
    "            with torch.no_grad():\n",
    "                apply_seq_mse(fp_prepared_model, quantsim, wiki_train_dataloader, params, modules_to_exclude=modules_to_exclude)\n",
    "\n",
    "\n",
    "    del fp_prepared_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.8 Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from aimet_torch.v2.experimental.quantsim_utils import clip_weights_to_7f7f\n",
    "\n",
    "def _calibration_forward_fn(sim_model, kwargs):\n",
    "\n",
    "    model.model = sim_model\n",
    "    data_loader = kwargs['data_loader']\n",
    "    max_iterations = kwargs['num_batches']\n",
    "    for batch_id, batch in enumerate(tqdm(data_loader, total=max_iterations)):\n",
    "        if batch_id < max_iterations:\n",
    "            model(input_ids=batch['input_ids'].to(device=torch.device('cuda')), \n",
    "                    num_slices=num_slices)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "kwargs = {\n",
    "    'data_loader': base_calibration_dataloader,\n",
    "    'num_batches': 200\n",
    "}\n",
    "\n",
    "with event_marker(\"compute encoding\", flush_ram=True):\n",
    "    with place_model(quantsim.model, \"cuda\"):\n",
    "        with torch.no_grad():\n",
    "            quantsim.compute_encodings(_calibration_forward_fn, kwargs)\n",
    "\n",
    "clip_weights_to_7f7f(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.9 Apply Activation Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clipping(quantsim, clamp_val):\n",
    "    from aimet_torch.v2.nn.base import BaseQuantizationMixin as QUANTIZED_MODULE\n",
    "\n",
    "    def _clip_and_recompute_encodings(quantizer, name, clamp_val):\n",
    "        if not quantizer.is_initialized():\n",
    "            return\n",
    "        qmin = quantizer.min.min()\n",
    "        qmax = quantizer.max.max()\n",
    "        if qmin < -clamp_val or qmax > clamp_val:\n",
    "            quantizer.min.data = torch.clamp(quantizer.min, -clamp_val, clamp_val)\n",
    "            quantizer.max.data = torch.clamp(quantizer.max, -clamp_val, clamp_val)\n",
    "\n",
    "            print(f\"{name} activation clamping... before: {qmin}, {qmax} | after: {quantizer.min.min().item()}, {quantizer.max.max().item()}\")\n",
    "\n",
    "    # Apply activation clipping\n",
    "    for name, module in quantsim.model.named_modules():\n",
    "        if isinstance(module, QUANTIZED_MODULE):\n",
    "            for quantizer in module.output_quantizers:\n",
    "                if quantizer:\n",
    "                    _clip_and_recompute_encodings(quantizer, name + \" | output quantizer\", clamp_val)\n",
    "            for quantizer in module.input_quantizers:\n",
    "                if quantizer:\n",
    "                    _clip_and_recompute_encodings(quantizer, name + \" | input quantizer\", clamp_val)\n",
    "\n",
    "if clamp_val is not None:\n",
    "    apply_clipping(quantsim, int(clamp_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.10 Eval KV Cache sim on Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    with event_marker(\"KV cache sim with base model eval\", flush_ram=True):\n",
    "        with place_model(quantsim.model, torch.device(\"cuda\")):\n",
    "            model.model = quantsim.model\n",
    "            sim_ppl = llm_evaluate_ppl_with_dataloader(model=model, dataloader=wiki_test_dataloader)\n",
    "\n",
    "    print(f\"ppl score of KVCACHE sim with base model = {sim_ppl}\")\n",
    "    print(f\"Diff between orig ppl and kvcache sim ppl = {orig_ppl - sim_ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    # Recipe_logger: Log the ppl for qsim model and dump the cumulative logs to a JSON file.\n",
    "    llm_lib_log_metric(ModelType.qsim_model, Metric.ppl, sim_ppl, model_name=\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export\n",
    "the pipeline call below would export onnx model, encodings and test vector for KVCache model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.1 Export Onnx and Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.onnx_utils import OnnxExportApiArgs\n",
    "\n",
    "# Get input names and output names. This is different from the input names and output names we created for model preparation. \n",
    "# The reason for this difference stems from the fact that we want the prepared model to have inputs and outputs named similar to original HF model\n",
    "# ONNX does not allow tupling the inputs or outputs and we want to give meaningful names to the input and output tensors in the ONNX graph\n",
    "input_names, output_names = llm_model_input_output_names(llm_config.num_hidden_layers, use_position_embedding_input=True, separate_tuple_input_output=True)\n",
    "\n",
    "def _get_anchor_buffer_names(sfx, n_layers):\n",
    "    return [f'anchor_buffer_{i}_{sfx}' for i in range(n_layers)]\n",
    "    \n",
    "\n",
    "if enable_right_padding:\n",
    "    input_names += [\"cache_index\"]\n",
    "\n",
    "if enable_fp16:\n",
    "    # Convert FP16 model back to FP32 for ONNX export\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    model.float()\n",
    "\n",
    "onnx_api_args = OnnxExportApiArgs(input_names=input_names, output_names=output_names, opset_version=17)\n",
    "\n",
    "base_filename_prefix = f\"{model_name}_base\"\n",
    "\n",
    "onnx_utils.RESTORE_ONNX_MODEL_INITIALIZERS = True\n",
    "\n",
    "dummy_input = get_dummy_data(device = \"cpu\", dtype = model.dtype,\n",
    "                             return_dict = True)\n",
    "\n",
    "base_onnx_dir = os.path.join(output_dir, 'base', 'onnx')\n",
    "os.makedirs(base_onnx_dir, exist_ok=True)\n",
    "\n",
    "sig = inspect.signature(prepared_model.forward)\n",
    "dummy_input_sorted = {}\n",
    "for key in list(sig.parameters.keys()):\n",
    "    dummy_input_sorted[key] = dummy_input[key]\n",
    "dummy_input = dummy_input_sorted\n",
    "dummy_input = tuple(list(dummy_input.values()))\n",
    "\n",
    "with event_marker(f\"KVCache export onnx and encodings\", flush_ram=True):\n",
    "    with torch.no_grad():\n",
    "        with place_model(quantsim.model, torch.device(\"cpu\")):\n",
    "            quantsim.export(base_onnx_dir, base_filename_prefix, dummy_input, onnx_export_args=onnx_api_args,\n",
    "                            export_model=True, filename_prefix_encodings=base_filename_prefix)\n",
    "\n",
    "# Exporting Tokenizer\n",
    "tokenizer_dir = os.path.join(output_dir, 'tokenizer')\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save_pretrained(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 8.2 Generating test vectors for QNN SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.llm.test_vectors import generate_test_vectors\n",
    "\n",
    "test_vector_layers = [\n",
    "    \"model_embed_tokens_Gather\",\n",
    "    \"model_layers_\\\\d+_Add_1\"\n",
    "]\n",
    "\n",
    "num_test_vectors = 1\n",
    "\n",
    "with event_marker(\"generate base model test vectors\"):\n",
    "    with place_model(quantsim.model, torch.device(\"cuda\")):\n",
    "        for index, batch in enumerate(wiki_train_dataloader):\n",
    "            if index >= num_test_vectors:\n",
    "                break\n",
    "            input_ids_slice = batch['input_ids'][..., :ARN].to(device=torch.device('cuda'))\n",
    "            attn_mask_slice = torch.ones((input_ids_slice.shape[0], ARN), dtype=torch.long, device=torch.device('cuda'))\n",
    "            position_ids_slice = torch.cumsum(attn_mask_slice, dim=1) - 1\n",
    "            outputs = {'past_key_values': None}\n",
    "            model_inputs = adapted_model_prepare_inputs_for_static_shapes(model, input_ids_slice=input_ids_slice, \n",
    "                                                                          attn_mask_slice=attn_mask_slice, \n",
    "                                                                          position_ids_slice=position_ids_slice,\n",
    "                                                                          outputs=outputs)\n",
    "            generate_test_vectors(sim=quantsim, model_inputs=model_inputs, output_dir=os.path.join(output_dir, 'base'), batch_index=index, test_vector_layers=test_vector_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from aimet_torch.pro.utils.profiler import EventProfiler\n",
    "from genai_lib.common.debug.recipe_logger import dump_logs_to_json\n",
    "\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(output_dir, 'profiling_stats.json'))\n",
    "dump_logs_to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
