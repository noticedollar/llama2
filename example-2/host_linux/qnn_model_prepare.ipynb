{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5549491",
   "metadata": {},
   "source": [
    "# QNN Model Prepare on Linux\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK allows clients to run ML models on HTP hardware. The following steps describe how to prepare the LLaMa2 models on Linux platforms for Windows.\n",
    "\n",
    "Before continuing, ensure all steps from [README](README.md) are completed.\n",
    "\n",
    "This document uses the term Qualcomm Neural Network (QNN) and Qualcomm AI Engine Direct SDK interchangeably.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "1. Qualcomm AI Engine Direct SDK (with Ubuntu Linux support)\n",
    "2. Ubuntu 22.04 installation with required packages for QNN Tools\n",
    "3. This notebook could be executed with Anaconda (with the supplied environment.yaml) or a virtual environment (venv)\n",
    "4. LLaMA `.onnx` files and their corresponding AIMET encodings and safetensors (generated via AIMET workflow in example1)\n",
    "\n",
    "This work flow assumes that you have generated the model artifacts following the AIMET LLaMa  workflow (example1):\n",
    "\n",
    "-   LLaMa2 model and its AIMET encodings\n",
    "-   `.pkl` file per network - a numpy object array saved as a Python pickle that contains data that is required as part of the model conversion step.\n",
    "\n",
    "![dir_struct](../jupyter_notebook_assets/nb1_output_dir_contents.png \"Overall directory Structure from notebook 1\") ![onnx_dir_struct](../jupyter_notebook_assets/onnx_dir_structure.png \"Snapshot of file contents of onnx folder from notebook 1\")\n",
    "\n",
    "# Workflow\n",
    "\n",
    "All the models and encodings are processed independently via different executable QNN utilities available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "To prepare LLaMa2 models for inference, the QNN executable utilities require an Ubuntu 22.04 environment\n",
    "\n",
    "1. Split the onnx model into several small onnx models.\n",
    "2. Apply MHA2SHA transformation to convert all attention block MHAs to SHAs. \n",
    "3. Convert the `.onnx` files to their equivalent QNN representation.\n",
    "4. Generate the QNN model quantized libraries.\n",
    "5. Import adapters and safetensors for the DLC format\n",
    "6. Generate the QNN context binaries for the QNN HTP backend.\n",
    "\n",
    "After preparing the LLaMa2 models for inference, the next step is to execute the QNN context binaries for inference on a Snapdragon Windows device.\n",
    "\n",
    "![QNN Work flow](../jupyter_notebook_assets/qnn-lora-v3-workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b089024",
   "metadata": {},
   "source": [
    "## Set up the Qualcomm AI Engine Direct SDK\n",
    "\n",
    "The following steps configure the Qualcomm AI Engine Direct SDK, which enables running LLaMA on the device.\n",
    "Execute the following on an Ubuntu 22.04 terminal.\n",
    "\n",
    "**NOTE:** These steps require sudo or root privileges.\n",
    "\n",
    "1. After setting up Python and pip in Ubuntu, check QNN tool dependencies.\n",
    "2. Set the `QNN_SDK_ROOT` environment variable to the location of the Qualcomm AI Engine Directory. For **Linux**, `export QNN_SDK_ROOT=\"./assets/qnn\"`\n",
    "3. Check and install Linux dependencies.\n",
    "\n",
    "    ```\n",
    "    source $QNN_SDK_ROOT/bin/check-linux-dependency.sh\n",
    "    sudo apt-get install -y libtinfo5\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a8d95",
   "metadata": {},
   "source": [
    "### Install the required python packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f70fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6fd1a",
   "metadata": {},
   "source": [
    "## Set up models and Qualcomm AI Engine Direct SDK variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def is_true(val: str):\n",
    "    return val.lower() in (\"true\", \"1\", \"t\")\n",
    "\n",
    "\n",
    "# setup whether using multithread or single thread to compile\n",
    "PARALLEL = is_true(os.getenv(\"PARALLEL\", \"True\"))\n",
    "\n",
    "# setup Target platform and its generation\n",
    "TARGET_PLATFORM = os.getenv(\"TARGET_PLATFORM\", \"Windows\").capitalize()\n",
    "PLATFORM_GEN = int(os.getenv(\"PLATFORM_GEN\", 2)) \n",
    "\n",
    "# Get source folder\n",
    "SRC_DIR = str(Path(\".\").resolve())\n",
    "\n",
    "# Set Work Folder, where You want to save outputs of NB2 (this NB)\n",
    "WORK_DIR = Path(os.getenv(\"OUTPUT_DIR\", os.getcwd()))\n",
    "# All the outputs will be saved in assets dir\n",
    "ASSETS_DIR = WORK_DIR / \"assets\"\n",
    "\n",
    "# Set up environment variable to reference Input network and SDK\n",
    "MODELS_DIR = Path(os.getenv(\"INPUT_MODEL_DIR\", ASSETS_DIR / \"models\" ))\n",
    "QNN_SDK_ROOT = os.getenv(\"QNN_SDK_ROOT\", ASSETS_DIR / \"qnn\")\n",
    "\n",
    "# Check path to LLAMA_MODELS and QNN_SDK_ROOT\n",
    "assert QNN_SDK_ROOT.exists(), f\"{QNN_SDK_ROOT=} path does not exist\"\n",
    "assert MODELS_DIR.exists(), f\"{MODELS_DIR=} path does not exist\"\n",
    "\n",
    "# These are the AR, CL and onnx name of the model exported from NB1\n",
    "EXPORT_AR = int(os.getenv(\"EXPORT_AR\", 2073))\n",
    "EXPORT_CONTEXT_LENGTH = int(os.getenv(\"EXPORT_CONTEXT_LENGTH\", 3073))\n",
    "ONNX_NAME = os.getenv(\"ONNX_NAME\", \"llamav2_base\")\n",
    "\n",
    "# These are the AR, CLs and num_splits we want to be output of model preparation from NB2 (this NB)\n",
    "CL_LIST = sorted(list(map(int, os.getenv(\"CL_LIST\", \"512,1024,2048,3072,4096\").split(\",\")))) #1024,2048,3072,4096\n",
    "ARNS = sorted(list(map(int, os.getenv(\"ARNS\", \"32,128\").split(\",\"))), reverse=True)\n",
    "\n",
    "NUM_SPLITS = int(os.getenv(\"NUM_SPLITS\", 4))\n",
    "\n",
    "# Setting up varibale to let split_onnx know if embedding and lm_head require its own seperate split\n",
    "SPLIT_EMBEDDING = is_true(os.getenv(\"SPLIT_EMBEDDING\", \"False\"))\n",
    "SPLIT_LMHEAD = is_true(os.getenv(\"SPLIT_LMHEAD\", \"False\"))\n",
    "\n",
    "# Set SSD params\n",
    "SSD_PARAMS_FILE = os.getenv(\"SSD_PARAMS_FILE\", None)\n",
    "assert (\n",
    "    not SSD_PARAMS_FILE or os.path.exists(SSD_PARAMS_FILE) == True\n",
    "), \"SSD_PARAMS_FILE path does not exist\"\n",
    "\n",
    "# Set Context Bin Generation handler\n",
    "EMBEDDING_ON_CPU = is_true(os.getenv(\"EMBEDDING_ON_CPU\", \"False\"))\n",
    "if EMBEDDING_ON_CPU and not SPLIT_EMBEDDING:\n",
    "    SPLIT_EMBEDDING = True\n",
    "    print(f\"WARNING! {EMBEDDING_ON_CPU=} requires {SPLIT_EMBEDDING=}. Setting {SPLIT_EMBEDDING=}.\")\n",
    "\n",
    "\n",
    "ENABLE_NATIVE_KV = is_true(os.getenv(\"ENABLE_NATIVE_KV\", \"True\"))\n",
    "\n",
    "os.environ[\"QNN_SDK_ROOT\"] = str(QNN_SDK_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e32d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "sys.path.extend(\n",
    "    [\n",
    "        SRC_DIR + \"/../G2G\",\n",
    "        SRC_DIR + \"/../G2G/split_onnx_utils\",\n",
    "        SRC_DIR + \"/..\",\n",
    "        SRC_DIR + \"/../..\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "from utilities.nsptargets import NspTargets\n",
    "from utilities.profiler import event_marker\n",
    "import utils\n",
    "\n",
    "# Set up nsp target specification\n",
    "# Android GEN4 and GEN5 is supported for this notebook\n",
    "nsp_target = eval(f\"NspTargets.{TARGET_PLATFORM}.GEN{PLATFORM_GEN}\")\n",
    "soc_id = nsp_target.soc_id\n",
    "dsp_arch = nsp_target.dsp_arch\n",
    "\n",
    "if ENABLE_NATIVE_KV:\n",
    "    supported_ARs = (32, 128)\n",
    "    unsupported_ARs = [arn for arn in ARNS if arn not in supported_ARs]\n",
    "    if len(unsupported_ARs) > 0:\n",
    "        raise ValueError(\n",
    "            f\"ERROR: Native KV Optimization only supported for AR32 and AR128, unsupported AR found: {unsupported_ARs}\"\n",
    "        )\n",
    "\n",
    "SPLITS = range(1, NUM_SPLITS + 1)\n",
    "ARN_CL_LIST = list(itertools.product(ARNS, CL_LIST))\n",
    "FULLTASKLIST = list(itertools.product(ARNS, CL_LIST, SPLITS))\n",
    "\n",
    "print(\"All task list:\", FULLTASKLIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12380716",
   "metadata": {},
   "source": [
    "### Set up environment variables for the Qualcomm AI Direct SDK tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_env = os.environ.copy()\n",
    "qnn_env[\"QNN_SDK_ROOT\"] = str(QNN_SDK_ROOT)\n",
    "qnn_env[\"PYTHONPATH\"] = f\"{QNN_SDK_ROOT}/benchmarks/QNN/:{QNN_SDK_ROOT}/lib/python\"\n",
    "qnn_env[\"PATH\"] = f\"{QNN_SDK_ROOT}/bin/x86_64-linux-clang:{qnn_env['PATH']}\"\n",
    "qnn_env[\"LD_LIBRARY_PATH\"] = f\"{QNN_SDK_ROOT}/lib/x86_64-linux-clang\"\n",
    "qnn_env[\"HEXAGON_TOOLS_DIR\"] = f\"{QNN_SDK_ROOT}/bin/x86_64-linux-clang\"\n",
    "os.environ = qnn_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ef8c1",
   "metadata": {},
   "source": [
    "# Prepare LLaMa2 models for Inference\n",
    "\n",
    "The following section uses the Qualcomm AI Engine Direct SDK to prepare LLaMa2 models for on-target inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc865b8",
   "metadata": {},
   "source": [
    "### Export to desired ARn\n",
    "\n",
    "Expected execution time: ~10 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9676413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import change_hardcoding\n",
    "\n",
    "\n",
    "def gen_ar(ARN_CL_LIST, scoring_net=False):\n",
    "    arn, CL = ARN_CL_LIST\n",
    "\n",
    "    fix_list = [\n",
    "        f\" {EXPORT_AR},{arn}\",\n",
    "        f\" -{EXPORT_AR},-1\",\n",
    "        f\" {EXPORT_CONTEXT_LENGTH},{CL}\",\n",
    "        f\" {EXPORT_CONTEXT_LENGTH-EXPORT_AR},{CL-arn}\",\n",
    "    ]\n",
    "\n",
    "    model_dir = MODELS_DIR\n",
    "    output_path = ASSETS_DIR / f\"ar{arn}_cl{CL}\" / \"src\"\n",
    "\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    change_hardcoding.execute(str(model_dir), str(output_path), fix_list)\n",
    "\n",
    "\n",
    "with event_marker(f\"prepare-export(ARn -> ARx) {ONNX_NAME}\"):\n",
    "    with concurrent.futures.ProcessPoolExecutor(\n",
    "        max_workers=len(ARN_CL_LIST) if PARALLEL else 1\n",
    "    ) as executor:\n",
    "        results = executor.map(gen_ar, ARN_CL_LIST)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)\n",
    "print(f\"Prepare AR export done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8b0b5",
   "metadata": {},
   "source": [
    "## Preprocess ONNX\n",
    "\n",
    "Prior to utilizing the QNN tool chain to compile and generate the context binary for LLaMA we need to split the model and generate the following artifacts\n",
    "\n",
    "-   ONNX file for each split of the model\n",
    "-   input vectors for each split\n",
    "-   golden output vectors for each split\n",
    "\n",
    "We need to specify the following parameters to proceed with execution of the notebook and generate all necessary artifacts\n",
    "\n",
    "-   number of splits of the model\n",
    "-   path to LLaMA onnx file\n",
    "-   path to LLaMA encodings file\n",
    "-   path to \\*.pkl files\n",
    "\n",
    "![Split](../jupyter_notebook_assets/ModelSplit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df707d",
   "metadata": {},
   "source": [
    "### Split Onnx export\n",
    "\n",
    "This step splits a model into multiple parts based on the number of splits specified.\n",
    "\n",
    "Expected execution time: ~10 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e7406",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_split(task):\n",
    "    arn, cl = task\n",
    "    target_model_name = f\"ar{arn}_cl{cl}\"\n",
    "    model_dir = ASSETS_DIR / target_model_name\n",
    "\n",
    "\n",
    "    model_name = ONNX_NAME\n",
    "    encodings_fname = f\"{ONNX_NAME}.encodings\"\n",
    "    lora_importer_config = None\n",
    "\n",
    "    print(f\"Starting {model_name}.onnx for {arn=} {cl=}\")\n",
    "    utils.split_onnx(\n",
    "        onnxfile=str(model_dir / \"src\" / \"onnx\" / f\"{model_name}.onnx\"),\n",
    "        encoding_file=str(model_dir / \"src\" / \"onnx\" / encodings_fname),\n",
    "        pickle_filedir=str(model_dir / \"src\" / \"test_vectors\"),\n",
    "        modelname=target_model_name,\n",
    "        num_splits=NUM_SPLITS,\n",
    "        split_embedding=SPLIT_EMBEDDING,\n",
    "        split_lmhead=SPLIT_LMHEAD,\n",
    "        embed_ssd_params_file=SSD_PARAMS_FILE,\n",
    "        using_qairt_workflow=True,\n",
    "        output_dir=model_dir,\n",
    "        lora_importer_config=lora_importer_config,\n",
    "    )\n",
    "    print(f\"Completed {model_name}.onnx for {arn=} {cl=}\")\n",
    "\n",
    "\n",
    "with event_marker(f\"split-onnx {ONNX_NAME}\"):\n",
    "    with concurrent.futures.ProcessPoolExecutor(\n",
    "        max_workers=len(ARN_CL_LIST) if PARALLEL else 1\n",
    "    ) as executor:\n",
    "        results = executor.map(thread_split, ARN_CL_LIST)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "print(f\"All onnx model splitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb7eb16",
   "metadata": {},
   "source": [
    "### Convert attention layers from MHA to SHA\n",
    "\n",
    "The `mha2sha-onnx-converter` tool converts a model from MHA representation to its equivalent SHA representation. The encoding files generated from the AIMET workflow are provided as an input to this step via the `--exported-model-encoding-path` option.\n",
    "\n",
    "This step generates a new `.onnx` file that represents the model in SHA format.\n",
    "\n",
    "Expected execution time: ~10 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e3343",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MHA2SHA_ROOT = f\"{SRC_DIR}/../G2G/MHA2SHA\"\n",
    "g2g_env = os.environ.copy()\n",
    "g2g_env[\"PYTHONPATH\"] = os.pathsep.join(\n",
    "    [g2g_env.get(\"PYTHONPATH\", \"\"), os.path.join(MHA2SHA_ROOT, \"src/python\")]\n",
    ")\n",
    "g2g_env[\"PATH\"] = os.pathsep.join([g2g_env.get(\"PATH\", \"\"), os.path.join(MHA2SHA_ROOT, \"bin\")])\n",
    "print(f\"MHA2SHA tool root set to: {MHA2SHA_ROOT}\")\n",
    "\n",
    "\n",
    "def thread_mha2sha(task):\n",
    "    arn, CL, split = task\n",
    "\n",
    "    if SPLIT_EMBEDDING and split == 1:\n",
    "        print(\"As first split only include embedding layer, so let's skip first split\")\n",
    "        return\n",
    "    elif SPLIT_LMHEAD and split == NUM_SPLITS:\n",
    "        print(\"As last split only include lm_head, so let's skip last split\")\n",
    "        return\n",
    "\n",
    "    artifacts_dir = ASSETS_DIR / f\"ar{arn}_cl{CL}\"\n",
    "    split_work_dir = artifacts_dir / f\"{split}_of_{NUM_SPLITS}\"\n",
    "    out_dir = split_work_dir / \"sha_output\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    name = f\"ar{arn}_cl{CL}_{split}_of_{NUM_SPLITS}\"\n",
    "\n",
    "\n",
    "    print(f\"mha2sha-onnx-converter {name} running...\")\n",
    "    args = [\n",
    "        \"mha2sha-onnx-converter\",\n",
    "        *[\"--sha-export-path\", str(out_dir)],\n",
    "        *[\"--model-name\", name],\n",
    "        *[\n",
    "            \"--exported-model-encoding-path\",\n",
    "            str(\n",
    "                artifacts_dir\n",
    "                / \"src\"\n",
    "                / \"onnx\"\n",
    "                /  (f\"{ONNX_NAME}.encodings\")\n",
    "            ),\n",
    "        ],\n",
    "        *[\n",
    "            \"--exported-model-path\",\n",
    "            str(artifacts_dir / \"split_onnx\" / f\"{name}.onnx\"),\n",
    "        ],\n",
    "        *[\"--base-llm\", \"llama2\"],\n",
    "        \"--mha-conv\",\n",
    "        \"--nchw-aligned\",\n",
    "    ]\n",
    "\n",
    "    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=g2g_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "    print(f\"mha2sha-onnx-converter {name} done.\")\n",
    "\n",
    "\n",
    "with event_marker(f\"mha2sha {ONNX_NAME}\"):\n",
    "    with concurrent.futures.ProcessPoolExecutor(\n",
    "        max_workers=len(FULLTASKLIST) if PARALLEL else 1\n",
    "    ) as executor:\n",
    "        results = executor.map(thread_mha2sha, FULLTASKLIST)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)\n",
    "print(f\"All mha2sha convert done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9146661",
   "metadata": {},
   "source": [
    "## Convert the model from ONNX representation to QNN DLC representation\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK `qairt-converter` tool converts a model from ONNX representation to its equivalent QNN DLC representation. The encoding files generated from the AIMET workflow are provided as an input to this step via the `â€“quantization_overrides model.encodings` option.\n",
    "\n",
    "This step generates a `.dlc` file that represents the model as a series of QNN API calls.\n",
    "\n",
    "Expected execution time: ~10 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25983b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_convert(task):\n",
    "\n",
    "    arn, cl, split = task\n",
    "\n",
    "    # When EMBEDDING_ON_CPU is True, we do not require to generate binaries for this split,\n",
    "    # hence, no conversion is required.\n",
    "    if EMBEDDING_ON_CPU and split == 1:\n",
    "        return\n",
    "\n",
    "    artifacts_dir = ASSETS_DIR / f\"ar{arn}_cl{cl}\"\n",
    "    split_work_dir = artifacts_dir / f\"{split}_of_{NUM_SPLITS}\"\n",
    "    out_dir = split_work_dir / \"converted_model\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    name = f\"ar{arn}_cl{cl}_{split}_of_{NUM_SPLITS}\"\n",
    "\n",
    "    if (SPLIT_EMBEDDING and split == 1) or (SPLIT_LMHEAD and split == NUM_SPLITS):\n",
    "        # mha2sha not applied to fisrt split (if SPLIT_EMBEDDING is set) or to last (if SPLIT_LMHEAD is set)\n",
    "        input_onnx = artifacts_dir / \"split_onnx\" / f\"{name}.onnx\"\n",
    "        quantization_overrides = (\n",
    "            artifacts_dir\n",
    "            / \"src\"\n",
    "            / \"onnx\"\n",
    "            / (f\"{ONNX_NAME}.encodings\")\n",
    "        )\n",
    "    else:\n",
    "        input_onnx = split_work_dir / \"sha_output\" / f\"{name}.onnx\"\n",
    "        quantization_overrides = split_work_dir / \"sha_output\" / f\"{name}.encodings\"\n",
    "\n",
    "    args = [\n",
    "        f\"{QNN_SDK_ROOT}/bin/x86_64-linux-clang/qairt-converter\",\n",
    "        *[\"--input_network\", str(input_onnx)],\n",
    "        *[\"--quantization_overrides\", str(quantization_overrides)],\n",
    "        *[\"--output_path\", str(out_dir / f\"{name}.dlc\")],\n",
    "    ]\n",
    "    for opt in utils.get_input_layout(str(input_onnx), using_qairt_workflow=True):\n",
    "        args += opt\n",
    "\n",
    "    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=qnn_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "\n",
    "    print()\n",
    "    print(f\"qairt-converter {name} done!\")\n",
    "\n",
    "\n",
    "with event_marker(f\"convert-onnx {ONNX_NAME}\"):\n",
    "    with concurrent.futures.ProcessPoolExecutor(\n",
    "        max_workers=len(FULLTASKLIST) if PARALLEL else 1\n",
    "    ) as executor:\n",
    "        results = executor.map(thread_convert, FULLTASKLIST)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "print(f\"All qairt-converter done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e40ecb",
   "metadata": {},
   "source": [
    "## Quantized QNN DLC model\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK `qairt-quantizer` compiles the model `.dlc` and input`.raw` files into a `model.quantized.dlc` file.\n",
    "\n",
    "The inputs to this stage are the input raw files & `model.dlc` generated in the previous step.\n",
    "\n",
    "Expected execution time: ~< 25 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a39542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_genlib(task):\n",
    "\n",
    "    arn, cl, split = task\n",
    "\n",
    "    # When EMBEDDING_ON_CPU is True, we do not require to generate binaries for this split,\n",
    "    # hence, quantization is not required.\n",
    "    if EMBEDDING_ON_CPU and split == 1:\n",
    "        return\n",
    "\n",
    "    artifacts_dir = ASSETS_DIR / f\"ar{arn}_cl{cl}\"\n",
    "    split_work_dir = artifacts_dir / f\"{split}_of_{NUM_SPLITS}\"\n",
    "    out_dir = split_work_dir / \"compiled_model\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    name = f\"ar{arn}_cl{cl}_{split}_of_{NUM_SPLITS}\"\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        [\n",
    "            f\"{QNN_SDK_ROOT}/bin/x86_64-linux-clang/qairt-quantizer\",\n",
    "            *[\"--act_bitwidth\", \"16\"],\n",
    "            *[\"--bias_bitwidth\", \"32\"],\n",
    "            *[\"--input_dlc\", str(split_work_dir / \"converted_model\" / f\"{name}.dlc\")],\n",
    "            *[\"--input_list\", str(artifacts_dir / f\"input_list_{name}.txt\")],\n",
    "            *[\"--output_dlc\", str(out_dir / f\"{name}.dlc\")],\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        env=qnn_env,\n",
    "    )\n",
    "\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "    print(f\"qairt-quantizer {name} done!\")\n",
    "\n",
    "\n",
    "with event_marker(f\"qairt-quantizer {ONNX_NAME}\"):\n",
    "    with concurrent.futures.ProcessPoolExecutor(\n",
    "        max_workers=len(FULLTASKLIST) if PARALLEL else 1\n",
    "    ) as executor:\n",
    "        results = executor.map(thread_genlib, FULLTASKLIST)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "print(f\"All qairt-quantizer done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a07bd0",
   "metadata": {},
   "source": [
    "## QNN HTP weight sharing context binary\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK `qnn-context-binary-generator` tool creates a QNN context binary applicable to the QNN HTP backend. This binary can be deployed to run on a Snapdragon 8 Gen4 device that runs Android. This step requires the ar128 and ar1 quantized DLCs from the previous step and the `libQnnHtp.so` library, available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "Provide additional options that pertain to the QNN HTP backend by passing the `libQnnHtpBackendExtensions.so` library that implements extensions for the QNN HTP backend. The library is available in the Qualcomm AI Engine Direct SDK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2133c",
   "metadata": {},
   "source": [
    "### Update config with absolute paths to SHA files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_importer_config(task):\n",
    "    arn, cl, split = task\n",
    "\n",
    "    artifacts_dir = ASSETS_DIR / f\"ar{arn}_cl{cl}\"\n",
    "    split_work_dir = artifacts_dir / f\"{split}_of_{NUM_SPLITS}\"\n",
    "    name = f\"ar{arn}_cl{cl}_{split}_of_{NUM_SPLITS}\"\n",
    "\n",
    "    lora_importer_config = artifacts_dir / \"src\" / \"onnx\" / \"lora_importer_config.yaml\"\n",
    "    with lora_importer_config.open(\"r\") as f:\n",
    "        lora_importer_config_data = yaml.safe_load(f)\n",
    "\n",
    "    for use_case in lora_importer_config_data[\"use_case\"]:\n",
    "        adaptor_name = use_case[\"name\"]\n",
    "        use_case[\"model_name\"] = str(split_work_dir / \"sha_output\" / f\"{name}.onnx\")\n",
    "        use_case[\"lora_weights\"] = str(\n",
    "            split_work_dir / \"sha_output\" / f\"{adaptor_name}_sha_weights.safetensor\"\n",
    "        )\n",
    "        use_case[\"quant_overrides\"] = str(\n",
    "            split_work_dir / \"sha_output\" / f\"{adaptor_name}_{name}.encodings\"\n",
    "        )\n",
    "        use_case[\"output_path\"] = str(split_work_dir / \"importer_output\")\n",
    "\n",
    "        for key in (\"model_name\", \"lora_weights\", \"quant_overrides\"):\n",
    "            assert Path(\n",
    "                use_case[key]\n",
    "            ).exists(), f\"{use_case[key]} does not exist for adaptor {adaptor_name}.\"\n",
    "\n",
    "    lora_importer_sha_config = split_work_dir / \"lora_importer_config.yaml\"\n",
    "    with lora_importer_sha_config.open(\"w\") as f:\n",
    "        yaml.dump(lora_importer_config_data, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb15589",
   "metadata": {},
   "source": [
    "### Define Htp Perf Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07439f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config_file(split, out_dir: Path, src_graphs, soc_id=69, dsp_arch=\"v79\"):\n",
    "    htp_config_path = out_dir / f\"HtpConfigFile_API_{split}.json\"\n",
    "    perf_config_path = out_dir / f\"PerfSetting_API_{split}.conf\"\n",
    "\n",
    "    htp_config = {\n",
    "        \"backend_extensions\": {\n",
    "            \"shared_library_path\": \"libQnnHtpNetRunExtensions.so\",\n",
    "            \"config_file_path\": str(perf_config_path),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    perf_config = {\n",
    "        \"graphs\": [\n",
    "            {\n",
    "                \"O\": 3.0,\n",
    "                \"vtcm_mb\": 8,\n",
    "                \"graph_names\": src_graphs,\n",
    "                \"fp16_relaxed_precision\": 0,\n",
    "                \"hvx_threads\": 6,\n",
    "            }\n",
    "        ],\n",
    "        \"devices\": [\n",
    "            {\n",
    "                \"soc_id\": int(soc_id),\n",
    "                \"dsp_arch\": dsp_arch,\n",
    "                \"cores\": [{\"perf_profile\": \"burst\", \"rpc_control_latency\": 100}],\n",
    "                \"pd_session\": \"unsigned\",\n",
    "            }\n",
    "        ],\n",
    "        \"context\": {\"weight_sharing_enabled\": len(src_graphs) > 1},\n",
    "        \"groupContext\": {\"share_resources\": True},\n",
    "        \"memory\": {\"mem_type\": \"shared_buffer\"},\n",
    "    }\n",
    "\n",
    "    with htp_config_path.open(\"w\") as f:\n",
    "        json.dump(htp_config, f, indent=4)\n",
    "\n",
    "    with perf_config_path.open(\"w\") as f:\n",
    "        json.dump(perf_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1829e94",
   "metadata": {},
   "source": [
    "### Native Format KV Optimization config generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_kv_format_config(split, folder: Path, data_format_config_name, scoring_net=False):\n",
    "    graphs_dict = {}\n",
    "    graphs_dict[\"graphs\"] = []\n",
    "    cl_list = CL_LIST\n",
    "    ARNs = ARNS\n",
    "    if scoring_net:\n",
    "        cl_list = SCORING_NET_CL_LIST\n",
    "        ARNs = [1]\n",
    "    for arn in ARNs:\n",
    "        for CL in cl_list:\n",
    "            model_artifact = ASSETS_DIR / f\"ar{arn}_cl{CL}\" / \"split_onnx\"\n",
    "            onnx_name = f\"ar{arn}_cl{CL}_{split}_of_{NUM_SPLITS}\"\n",
    "            onnxfile = model_artifact / f\"{onnx_name}.onnx\"\n",
    "\n",
    "            if scoring_net:\n",
    "                model_artifact = ASSETS_DIR / f\"{SCORING_NETWORK_ONNX_NAME}_cl{CL}\" / \"onnx\"\n",
    "                onnx_name = SCORING_NETWORK_ONNX_NAME\n",
    "                onnxfile = model_artifact / f\"{onnx_name}.onnx\"\n",
    "\n",
    "            input_names, output_names = utils.get_onnx_input_output_names(\n",
    "                str(onnxfile), deco_digit=False, using_qairt_workflow=True\n",
    "            )\n",
    "            tensors = [\n",
    "                {\n",
    "                    \"tensor_name\": name,\n",
    "                    \"dataFormat\": \"QNN_TENSOR_DATA_FORMAT_HMX_WEIGHT_LAYOUT\",\n",
    "                }\n",
    "                for name in input_names + output_names\n",
    "                if \"key\" in name or \"value\" in name\n",
    "            ]\n",
    "            if len(tensors) > 0:\n",
    "                graphs_dict[\"graphs\"].append(\n",
    "                    {\n",
    "                        \"graph_name\": onnx_name,\n",
    "                        \"tensors\": tensors,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    with (folder / data_format_config_name).open(\"w\") as f:\n",
    "        json.dump(graphs_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ed265",
   "metadata": {},
   "source": [
    "### Compile context binary\n",
    "\n",
    "Expected execution time: ~< 3 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_adapter_configs(adapter_config_paths, merged_adapter_config_path):\n",
    "    merged_use_cases = []\n",
    "    for adapter_config_path in adapter_config_paths:\n",
    "        with adapter_config_path.open(\"r\") as f:\n",
    "            adapter_config_data = yaml.safe_load(f)\n",
    "        merged_use_cases.extend(adapter_config_data[\"use_case\"])\n",
    "\n",
    "    merged_adapter_config_data = {\n",
    "        \"use_case\": merged_use_cases,\n",
    "        \"share_adapters_between_graphs\": \"Yes\",\n",
    "    }\n",
    "    with merged_adapter_config_path.open(\"w\") as f:\n",
    "        yaml.dump(merged_adapter_config_data, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f30e5b",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_gen_ws_cb(split):\n",
    "\n",
    "    # When EMBEDDING_ON_CPU is True, we do not require to generate binaries for this split.\n",
    "    if EMBEDDING_ON_CPU and split == 1:\n",
    "        return\n",
    "\n",
    "    graph_list = []\n",
    "    dlc_list = []\n",
    "    adapter_config_paths = []\n",
    "    for ar, cl in ARN_CL_LIST:\n",
    "        split_dir = ASSETS_DIR / f\"ar{ar}_cl{cl}\" / f\"{split}_of_{NUM_SPLITS}\"\n",
    "\n",
    "        graph_name = f\"ar{ar}_cl{cl}_{split}_of_{NUM_SPLITS}\"\n",
    "        src_q_dlc = split_dir / \"compiled_model\" / f\"{graph_name}.dlc\"\n",
    "\n",
    "        graph_list.append(graph_name)\n",
    "        dlc_list.append(str(src_q_dlc))\n",
    "\n",
    "    ar_str = \"_\".join(f\"ar{x}\" for x in ARNS)\n",
    "    cl_str = \"_\".join(f\"cl{x}\" for x in CL_LIST)\n",
    "\n",
    "    out_dir = ASSETS_DIR / f\"{ar_str}_{cl_str}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    conf_dir = ASSETS_DIR / f\"{ar_str}_{cl_str}_conf_files\"\n",
    "    conf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    make_config_file(split, conf_dir, graph_list, soc_id, dsp_arch)\n",
    "\n",
    "    binary_file_name = f\"weight_sharing_model_{ar_str}_{cl_str}_{split}_of_{NUM_SPLITS}\"\n",
    "    if ENABLE_NATIVE_KV:\n",
    "        binary_file_name += \"_natKV\"\n",
    "\n",
    "    cmd = [\n",
    "        qnn_env[\"HEXAGON_TOOLS_DIR\"] + \"/qnn-context-binary-generator\",\n",
    "        \"--log_level=error\",\n",
    "        *[\"--backend\", \"libQnnHtp.so\"],\n",
    "        *[\"--model\", \"libQnnModelDlc.so\"],\n",
    "        *[\"--input_output_tensor_mem_type\", \"memhandle\"],\n",
    "        *[\"--config_file\", str(conf_dir / f\"HtpConfigFile_API_{split}.json\")],\n",
    "        *[\"--dlc_path\", \",\".join(dlc_list)],\n",
    "        *[\"--output_dir\", str(out_dir)],\n",
    "        *[\"--binary_file\", f\"{binary_file_name}.serialized\"],\n",
    "    ]\n",
    "\n",
    "\n",
    "    if ENABLE_NATIVE_KV:\n",
    "        data_format_config_name = f\"data_format_config_{split}_of_{NUM_SPLITS}.json\"\n",
    "        gen_kv_format_config(split, conf_dir, data_format_config_name)\n",
    "        cmd += [\"--data_format_config\", str(conf_dir / data_format_config_name)]\n",
    "\n",
    "    print(\" \".join(cmd))\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=qnn_env)\n",
    "    output, error = proc.communicate()\n",
    "    print(output.decode(), error.decode())\n",
    "    print(f\"#{split} weight sharing model generated.\")\n",
    "\n",
    "\n",
    "with event_marker(f\"context-binary {ONNX_NAME}\"):\n",
    "    with concurrent.futures.ProcessPoolExecutor(\n",
    "        max_workers=len(SPLITS) if PARALLEL else 1\n",
    "    ) as executor:\n",
    "        results = executor.map(thread_gen_ws_cb, SPLITS)\n",
    "        for result in results:\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "print(f\"All weight shared qnn-context-binary generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455bd8d",
   "metadata": {},
   "source": [
    "### Save profiling stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.profiler import EventProfiler\n",
    "\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(str(ASSETS_DIR / \"profiling_stats.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d964165",
   "metadata": {},
   "source": [
    "Upon completion of these steps to prepare models for inference, QNN context binaries are available in `./assets/artifacts`.\n",
    "The next step is to execute the prepared models (now represented as serialized context binaries)on a Snapdragon 8 Gen4 Android device using executable utilities available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
