{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA2SHA\n",
    "\n",
    "This notebook shows how to use MHA2SHA from a high level. MHA2SHA at its core is simply converting Multi Head Attetntion (MHA) ops to Single Head Attention (SHA), as well as, propogate AIMET encodings to the newly converted SHA model. MHA2SHA can be used pythonically and via the command line. This notebook will cover both of these for each block of information.\n",
    "\n",
    "#### Overall Notebook Flow\n",
    "This notebook covers the following:\n",
    "\n",
    "1. MHA2SHA setup\n",
    "2. Flag/Arg information\n",
    "\n",
    "#### Assumptions\n",
    "We will use a LLaMAv2 model that is an artifacts of the AI research notebooks. This comes with changes to the model that is not standard with a LLaMAv2 model straight from Hugging Face.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MHA2SHA Setup\n",
    "\n",
    "To setup MHA2SHA, there is a environment setup script called `env_setup.sh`. Running this will set paths accordingly. Below is how to run from the top level of MHA2SHA.\n",
    "\n",
    "```bash\n",
    "source bin/env_setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are in the root of MHA2SHA you can run this cell\n",
    "!source bin/env_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Flag/Arg Information\n",
    "\n",
    "At a minimum, MHA2SHA needs the following flags/args: `--model-name`, `--exported-model-path`(`model_or_path` in python). `--exported-model-encoding-path` is not mandatory as some models may not have encodings.\n",
    "\n",
    "Apart from the minimum, MHA2SHA has a lot of flags to cover many iterations of LLM/LVM models. A full list of these can be seen in the README. To make this easier, we provide a higher level flag `--base-llm`. This flag will map a LLM/LVM's base architecture to the necessary flags that are needed for MHA2SHA. For example, `--base-llm llama2` will implicitly turn on flags such as `--handle-rope-ops`, `--llm-model`, etc. If a user provides a flag that contradicts a default flag for the LLM/LVM base architecture, we warn the user but use the explicitly provided flag. For a list of `--base-llm` architectures supported, see the README.\n",
    "\n",
    "Below will show how to run a LLaMAv2 model through the command line and pythonically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command line variation of running MHA2SHA easy to use. Below highlights how to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "mha2sha-onnx-converter \\\n",
    "--model-name example \\\n",
    "--sha-export-path [PATH-TO-EXPORT] \\\n",
    "--exported-model-path [PATH-TO-MODEL] \\\n",
    "--exported-model-encoding-path [PATH-TO-ENCODINGS] \\\n",
    "--base-llm llama2\n",
    "--mha-conv  # These are flags necessary if the model is an artifact from AI Research notebooks\n",
    "--nchw-aligned # These are flags necessary if the model is an artifact from AI Research notebooks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pythonic way of running MHA2SHA is similar to the command line. The difference here is that the `__init__` of the MHA2SHAConverter takes positional arguments for the minimum arguments listed above, followed by keyword arguments for all other additional flags. It may be easier to make a dictionary of flags needed and unpacking it in the `__init__`.\n",
    "\n",
    "The Python API will convert the model and give back the converted model and a verification status. This verification status is `True` if the original MHA models logits matched the SHA converted models logits otherwise it's `False`. Please see `--no-verification` in the README for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mha2sha.converter import MHA2SHAConverter\n",
    "\n",
    "flags = {\n",
    "    \"exported_model_encoding_path\": \"path/to/encodings\",\n",
    "    \"base_llm\": \"llama2\",\n",
    "    \"mha_conv\": True, # These are flags necessary if the model is an artifact from AI Research notebooks\n",
    "    \"nchw_aligned\": True # These are flags necessary if the model is an artifact from AI Research notebooks\n",
    "}\n",
    "\n",
    "converter = MHA2SHAConverter(\n",
    "    model_name=\"example\",\n",
    "    sha_export_path=\"path/to/export\",\n",
    "    model_or_path=\"path/to/model\",\n",
    "    **flags\n",
    ")\n",
    "sha_model, verification_status = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Hopefully this notebook was useful in understanding how to integrate MHA2SHA into your pipeline.\n",
    "\n",
    "For additional resources:\n",
    "- View the README\n",
    "- Contact the contributors listed in the README"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
